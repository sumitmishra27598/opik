{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyNmH7GmU9YKD0HULxtLDlac"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Opik"],"metadata":{"id":"b81l6ph2L-19"}},{"cell_type":"markdown","source":["The Opik platform enables you to log, view, and assess your LLM traces throughout development and production. By utilizing the platform alongside our LLM as a Judge evaluator, you can pinpoint and resolve issues in your LLM application."],"metadata":{"id":"7C_z4igkMBEZ"}},{"cell_type":"markdown","source":["You can access Opik through Comet's Managed Cloud offering or self-host it on your own infrastructure. Self-hosting allows you to utilize all Opik features, including tracing and evaluation, though user management features will not be available.\n","\n","If you opt for self-hosting, you have two deployment options:\n","\n","1. **Local Installation:** Ideal for getting started, but not suitable for production.\n","2. **Kubernetes Installation:** A production-ready Opik platform that operates on a Kubernetes cluster.\n","\n","Alternatively, you can use Opik by simply installing it and setting up your API key."],"metadata":{"id":"8bH2T9OxlBUh"}},{"cell_type":"markdown","source":["## Tracing"],"metadata":{"id":"3eyWtoEmnUCI"}},{"cell_type":"markdown","source":["### Logging Traces\n","\n","You can log traces to the Comet LLM Evaluation platform using either a REST API or the Opik Python SDK.\n","\n","**Using the Python SDK**  \n","First, install the SDK and configure it with your Opik API key or local deployment settings. This setup process is straightforward and guided.\n","\n","Once set up, you can log traces through Comet's integrations, function decorators, or manually.\n","\n","```\n","import opik\n","\n","opik.configure(use_local=False)\n","```\n","\n","**Function Decorators**  \n","The simplest way to log with Opik is by using function decorators. This method integrates seamlessly with your existing LLM application and is recommended alongside Comet's integrations for optimal results.\n","```\n","from opik import track\n","\n","@track\n","def preprocess_input(text):\n","    return text.strip().lower()\n","```\n","\n","If you’re defining LLM chains manually, you can use the track decorators to monitor LLM calls. By default, these decorators capture the function's input and output, but you can customize what is recorded.\n","```\n","@track(capture_input=False, capture_output=False)\n","```\n","\n","**Manual Logging**  \n","For more control, you can log traces and spans manually using the Comet client. This allows you to create traces and add spans for various operations, tracking inputs and outputs as needed.\n","```\n","from opik import Opik\n","\n","client = Opik(project_name=\"test\")\n","\n","# Create a trace\n","trace = client.trace(\n","    name=\"my_trace\",\n","    input={\"user_question\": \"Hello, how are you?\"},\n","    output={\"response\": \"Comment ça va?\"}\n",")\n","```\n","\n","**Updating Trace and Span Attributes**  \n","You can modify the attributes of traces and spans during execution to update metadata or log scores.\n","\n","**Logging Scores**  \n","Scores can be logged to traces and spans using dedicated methods, allowing you to capture feedback on the quality and coherence of responses.\n","\n","**Advanced Usage**  \n","To enhance performance, logging occurs in a background thread. If you want to ensure all traces are sent to Comet before exiting your program, use the flush method.\n","\n","**Logging Distributed Traces**\n","In complex LLM applications, tracking traces across multiple services is essential. Comet supports distributed tracing seamlessly when using function decorators, employing a mechanism similar to OpenTelemetry.\n","\n","For more detail, refer [this](https://www.comet.com/docs/opik/tracing/log_traces)."],"metadata":{"id":"WpcrkvHHl4OH"}},{"cell_type":"markdown","source":["### Annotating Traces\n","\n","Annotating traces is essential for evaluating and enhancing your LLM-based applications. By systematically recording qualitative or quantitative feedback on specific interactions or entire conversation flows, you can:\n","\n","- Track performance over time\n","- Identify areas for improvement\n","- Compare different model versions or prompts\n","- Gather data for fine-tuning or retraining\n","- Provide stakeholders with concrete metrics on system effectiveness\n","\n","Opik enables you to annotate traces using either the SDK or the UI.\n","\n","**Annotating Traces through the UI**  \n","To annotate traces via the UI, navigate to the trace you wish to annotate on the traces page and click the \"Annotate\" button. This opens a sidebar where you can add your annotations. You can annotate both traces and spans, so be sure to select the correct span in the sidebar.\n","\n","**Annotating Traces through the SDK**  \n","You can also annotate traces and spans using the SDK, which is useful for incorporating evaluation or user feedback scores.\n","\n","**Logging Feedback Scores for Traces**  \n","You can log feedback scores for traces with the `log_traces_feedback_scores` method. Each score can include an optional reason field for clarity.\n","```\n","from opik import Opik\n","\n","client = Opik(project_name=\"my_project\")\n","\n","trace = client.trace(name=\"my_trace\")\n","\n","client.log_traces_feedback_scores(\n","    scores=[\n","        {\"id\": trace.id, \"name\": \"overall_quality\", \"value\": 0.85},\n","        {\"id\": trace.id, \"name\": \"coherence\", \"value\": 0.75},\n","    ]\n",")\n","```\n","\n","**Logging Feedback Scores for Spans**  \n","To log feedback for individual spans, use the `log_spans_feedback_scores` method. This allows you to capture detailed feedback on specific parts of your LLM application.\n","```\n","from opik import Opik\n","\n","client = Opik()\n","\n","trace = client.trace(name=\"my_trace\")\n","span = trace.span(name=\"my_span\")\n","\n","comet.log_spans_feedback_scores(\n","    scores=[\n","        {\"id\": span.id, \"name\": \"overall_quality\", \"value\": 0.85},\n","        {\"id\": span.id, \"name\": \"coherence\", \"value\": 0.75},\n","    ],\n",")\n","```\n","\n","Computing feedback scores can be challenging with Large Language Models (LLMs) due to their unstructured and non-deterministic outputs. To assist with this, Opik offers a range of built-in evaluation metrics."],"metadata":{"id":"LFzSnTzeoJr6"}},{"cell_type":"markdown","source":["## Evaluation"],"metadata":{"id":"NRTnHy-Mo5h4"}},{"cell_type":"markdown","source":["When working with LLM applications, the evaluation process can often slow down iteration. While manually reviewing outputs is possible, it’s inefficient and not scalable. Opik streamlines this by automating the evaluation of your LLM application.\n","\n","To effectively run evaluations in Opik, it’s essential to understand two key concepts:\n","\n","- **Dataset:** A dataset consists of a collection of samples used for evaluation. It stores the input and expected outputs for each sample, while the actual outputs from your LLM application are computed and scored during the evaluation.\n","\n","- **Experiment:** An experiment represents a single evaluation of your LLM application. During an experiment, each item in the dataset is processed, the output is generated by your LLM, and then the output is scored."],"metadata":{"id":"JsxoRC__skxO"}},{"cell_type":"markdown","source":["### Datasets\n","\n","The first step in automating the evaluation of your LLM application is to create a dataset, which is a collection of samples for evaluation. Each dataset consists of Dataset Items that store the input, expected output, and other metadata for individual samples.\n","\n","Given their importance in the evaluation process, teams often dedicate considerable time to curating and preparing their datasets. There are three primary ways to create a dataset:\n","\n","1. **Manually Curating Examples:** Start by manually selecting a set of examples based on your understanding of the application. Involve subject matter experts to enhance the quality of the dataset.\n","\n","2. **Using Synthetic Data:** If you lack sufficient data for a diverse set of examples, consider using synthetic data generation tools.\n","\n","3. **Leveraging Production Data:** If your application is already in production, you can enrich your dataset by using real-world data generated during operation. While this may not be the initial step, it can significantly enhance the dataset.\n","\n","If you're using Opik for production monitoring, you can easily add traces to your dataset by selecting them in the UI and choosing \"Add to dataset\" from the Actions dropdown.\n","\n","\n","#### Manage Datasets\n","Datasets are essential for tracking the test cases you want to evaluate your LLM on. Each dataset consists of DatasetItems, which include input, optional expected output, and metadata fields. You can create datasets through the following methods:\n","\n","1. **Python SDK:** Use the Python SDK to create a dataset and add items programmatically.\n","2. **Traces Table:** Incorporate existing logged traces (e.g., from a production application) into a dataset.\n","3. **Comet UI:** Manually create a dataset and add items directly through the Comet user interface.\n","\n","Once a dataset is created, you can run Experiments on it. Each Experiment evaluates your LLM application against the test cases in the dataset, utilizing an evaluation metric to report results back to the dataset."],"metadata":{"id":"-0E-o_Kh5K7o"}},{"cell_type":"markdown","source":["### Experiments\n","\n","Experiments are the foundational elements of the Opik evaluation framework. Each time you conduct a new evaluation, a new experiment is generated, consisting of two main components:\n","\n","#### Experiment Configuration\n","The configuration object for each experiment enables you to track metadata, such as the prompt template used. This is particularly useful for maintaining clarity about what has changed between iterations of an experiment, such as the model used, temperature settings, and other relevant parameters. You can easily compare the configurations of different experiments through the Opik UI to identify any differences.\n","\n","#### Experiment Items\n","Experiment items capture essential details for each dataset sample processed during an experiment. They include the input, expected output, actual output, and feedback scores. Each item is also linked to a trace, providing context for why a particular item received its score.\n","\n","In addition, you'll be able to view the average scores for each metric associated with the experiment, giving you a comprehensive overview of the evaluation results."],"metadata":{"id":"6kuuWFcN6wqX"}},{"cell_type":"markdown","source":["### Evaluating Your LLM Application\n","\n","Evaluating your LLM application helps ensure confidence in its performance. This evaluation typically occurs both during development and as part of application testing.\n","\n","The evaluation process consists of five steps:\n","\n","1. **Add Tracing:** Integrate tracing into your LLM application to log relevant data.\n","2. **Define the Evaluation Task:** Specify the task that maps the inputs to the expected outputs.\n","3. **Choose the Dataset:** Select the dataset on which you want to evaluate your application.\n","4. **Select Metrics:** Decide on the metrics you will use for evaluation.\n","5. **Create and Run the Evaluation Experiment:** Set up and execute the evaluation experiment based on the chosen components.\n","    To run an evaluation, you'll need to gather the following components:\n","\n","    1. **Dataset:** The dataset on which you want to conduct the evaluation.\n","    2. **Evaluation Task:** This defines how the inputs in the dataset are mapped to the outputs you wish to score. This is typically the LLM application you are developing.\n","    3. **Metrics:** The specific metrics you want to use for scoring the outputs of your LLM.\n","\n","By following these steps, you can effectively assess and improve the performance of your LLM application.\n","\n","\n","```\n","from opik import Opik, track, DatasetItem\n","from opik.evaluation import evaluate\n","from opik.evaluation.metrics import Equals, Hallucination\n","from opik.integrations.openai import track_openai\n","import openai\n","\n","# Define the task to evaluate\n","openai_client = track_openai(openai.OpenAI())\n","\n","MODEL = \"gpt-3.5-turbo\"\n","\n","# Add tracing to your application\n","@track\n","def your_llm_application(input: str) -> str:\n","    response = openai_client.chat.completions.create(\n","        model=MODEL,\n","        messages=[{\"role\": \"user\", \"content\": input}],\n","    )\n","\n","    return response.choices[0].message.content\n","\n","# Define the evaluation task\n","def evaluation_task(x: DatasetItem):\n","    return {\n","        \"input\": x.input['user_question'],\n","        \"output\": your_llm_application(x.input['user_question']),\n","        \"context\": your_context_retriever(x.input['user_question'])\n","    }\n","\n","@track\n","def your_context_retriever(input: str) -> str:\n","    return [\"...\"]\n","\n","\n","# Create a simple dataset\n","client = Opik()\n","try:\n","    dataset = client.create_dataset(name=\"your-dataset-name\")\n","    dataset.insert([\n","        {\"input\": {\"user_question\": \"What is the capital of France?\"}},\n","        {\"input\": {\"user_question\": \"What is the capital of Germany?\"}},\n","    ])\n","except:\n","    dataset = client.get_dataset(name=\"your-dataset-name\")\n","\n","# Define the metrics\n","hallucination_metric = Hallucination()\n","\n","# Create and Run the Evaluation Experiment\n","evaluation = evaluate(\n","    experiment_name=\"My experiment\",\n","    dataset=dataset,\n","    task=evaluation_task,\n","    scoring_metrics=[hallucination_metric],\n","    experiment_config={\n","        \"model\": MODEL\n","    }\n",")\n","```"],"metadata":{"id":"a8wQ2lug9RuU"}},{"cell_type":"markdown","source":["### Metrics Overview\n","\n","Opik offers a set of built-in evaluation metrics to assess the output of your LLM calls, categorized into two main types:\n","\n","1. **Heuristic Metrics:** These metrics are deterministic and typically statistical in nature.\n","2. **LLM as a Judge Metrics:** These are non-deterministic metrics that utilize an LLM to evaluate the output of another LLM.\n","\n","#### Built-in Evaluation Metrics\n","\n","| Metric           | Type                 | Description                                        |\n","|------------------|----------------------|----------------------------------------------------|\n","| **Equals**       | Heuristic            | Checks if the output exactly matches an expected string. |\n","| **Contains**     | Heuristic            | Verifies if the output contains a specific substring, with case sensitivity options. |\n","| **RegexMatch**   | Heuristic            | Checks if the output matches a specified regular expression pattern. |\n","| **IsJson**       | Heuristic            | Validates if the output is a valid JSON object.   |\n","| **Levenshtein**  | Heuristic            | Calculates the Levenshtein distance between the output and an expected string. |\n","| **Hallucination**| LLM as a Judge       | Identifies if the output contains any hallucinations. |\n","| **Moderation**   | LLM as a Judge       | Checks if the output contains harmful content.     |\n","| **AnswerRelevance** | LLM as a Judge    | Assesses if the output is relevant to the question. |\n","| **ContextRecall**| LLM as a Judge       | Evaluates if the output includes relevant context. |\n","| **ContextPrecision** | LLM as a Judge   | Measures the precision of the output in relation to the context. |\n","\n","You can also create your own custom metric; for more information, refer to the [this](https://www.comet.com/docs/opik/evaluation/metrics/custom_metric)."],"metadata":{"id":"_pyI3nTA6-6u"}},{"cell_type":"markdown","source":["## Integration"],"metadata":{"id":"hVbJHdIPsPpx"}},{"cell_type":"markdown","source":["Opik simplifies the process of logging, viewing, and evaluating your LLM traces by offering a range of integrations:\n","\n","| Integration   | Description                                |\n","|---------------|--------------------------------------------|\n","| **OpenAI**    | Log traces for all OpenAI LLM calls       |\n","| **LangChain** | Log traces for all LangChain LLM calls    |\n","| **LlamaIndex**| Log traces for all LlamaIndex LLM calls   |\n","| **Ollama**    | Log traces for all Ollama LLM calls       |\n","| **Predibase** | Fine-tune and serve open-source LLMs      |\n","| **Ragas**     | Evaluation framework for Retrieval Augmented Generation (RAG) pipelines |\n","\n","These integrations help you effectively manage your LLM applications."],"metadata":{"id":"EOSvzgvTpa9I"}},{"cell_type":"markdown","source":["### Example: Opik with Langchain for Text to SQL Query Generation"],"metadata":{"id":"-Xdwccm0JGGt"}},{"cell_type":"markdown","source":["Comet offers smooth integration with LangChain, enabling you to effortlessly log and track your LangChain applications.\n","\n","This example walks you through the process of generating SQL queries from natural language questions using LangChain and the Chinook database. The workflow involves creating a synthetic dataset of questions, building a LangChain to generate SQL queries, and automating the evaluation of those queries."],"metadata":{"id":"jr6H824vJRjh"}},{"cell_type":"markdown","source":["#### Prerequisites"],"metadata":{"id":"o5NI6bvsJWND"}},{"cell_type":"markdown","source":["Before you begin, ensure you have:\n","\n","*   An account on Comet to access the Opik platform.\n","*   An access to any LLM."],"metadata":{"id":"glA_zRMnJbT4"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kIh6oZPqweHA","executionInfo":{"status":"ok","timestamp":1726993724474,"user_tz":-330,"elapsed":27317,"user":{"displayName":"Sumit Mishra","userId":"03876200325202187761"}},"outputId":"6212b29b-b6b3-4978-c197-c66015039a99"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.5/185.5 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m57.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m375.6/375.6 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.4/386.4 kB\u001b[0m \u001b[31m23.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m58.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install --upgrade --quiet opik langchain langchain-community langchain-openai"]},{"cell_type":"markdown","source":["#### Setting Up API Keys"],"metadata":{"id":"_vpS2vvmKC-f"}},{"cell_type":"code","source":["import opik\n","\n","opik.configure(use_local=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z1aESoDO0Lde","executionInfo":{"status":"ok","timestamp":1726993755519,"user_tz":-330,"elapsed":12349,"user":{"displayName":"Sumit Mishra","userId":"03876200325202187761"}},"outputId":"1ac37463-0765-4764-f3c7-cbe594d91638"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["OPIK: Your Opik cloud API key is available at https://www.comet.com/api/my/settings/.\n"]},{"name":"stdout","output_type":"stream","text":["Please enter your Opik Cloud API key:··········\n","Do you want to use \"sumitmishra5504\" workspace? (Y/n)Y\n"]},{"output_type":"stream","name":"stderr","text":["OPIK: Saved configuration to a file: /root/.opik.config\n"]}]},{"cell_type":"code","source":["import os\n","import getpass\n","\n","if \"OPENAI_API_KEY\" not in os.environ:\n","    os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter your OpenAI API key: \")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C52wgXUO0WAl","executionInfo":{"status":"ok","timestamp":1726993782389,"user_tz":-330,"elapsed":3866,"user":{"displayName":"Sumit Mishra","userId":"03876200325202187761"}},"outputId":"a44357de-361d-44a3-c572-58ff71014346"},"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Enter your OpenAI API key: ··········\n"]}]},{"cell_type":"markdown","source":["#### Creating and Preparing Synthetic Dataset"],"metadata":{"id":"kyeiFZNwKGlG"}},{"cell_type":"markdown","source":["To generate a dataset of questions, we will utilize the OpenAI API. The goal is to create 25 diverse questions related to the Chinook database."],"metadata":{"id":"4mHv5NGcKevL"}},{"cell_type":"code","source":["# Download the relevant data\n","import os\n","from langchain_community.utilities import SQLDatabase\n","\n","import requests\n","import os\n","\n","url = \"https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\"\n","filename = \"./data/chinook/Chinook_Sqlite.sqlite\"\n","\n","folder = os.path.dirname(filename)\n","\n","if not os.path.exists(folder):\n","    os.makedirs(folder)\n","\n","if not os.path.exists(filename):\n","    response = requests.get(url)\n","    with open(filename, 'wb') as file:\n","        file.write(response.content)\n","    print(f\"Chinook database downloaded\")\n","\n","db = SQLDatabase.from_uri(f\"sqlite:///{filename}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5xezu7G09Xs","executionInfo":{"status":"ok","timestamp":1726993790938,"user_tz":-330,"elapsed":1489,"user":{"displayName":"Sumit Mishra","userId":"03876200325202187761"}},"outputId":"6aa465eb-a1b2-4278-cd29-57027067923e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Chinook database downloaded\n"]}]},{"cell_type":"markdown","source":["Then, use the OpenAI API to generate the questions. Ensure that each API call is tracked with the track_openai function from the opik library."],"metadata":{"id":"639JYeJZKyCm"}},{"cell_type":"code","source":["from opik.integrations.openai import track_openai\n","from openai import OpenAI\n","import json\n","\n","os.environ[\"OPIK_PROJECT_NAME\"] = \"langchain-integration-demo\"\n","client = OpenAI()\n","\n","openai_client = track_openai(client)\n","\n","prompt = \"\"\"\n","Create 25 different example questions a user might ask based on the Chinook Database.\n","\n","These questions should be complex and require the model to think. They should include complex joins and window functions to answer.\n","\n","Return the response as a json object with a \"result\" key and an array of strings with the question.\n","\"\"\"\n","\n","completion = openai_client.chat.completions.create(\n","  model=\"gpt-3.5-turbo\",\n","  messages=[\n","    {\"role\": \"user\", \"content\": prompt}\n","  ]\n",")\n","\n","print(completion.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aHfr7QhG4Frx","executionInfo":{"status":"ok","timestamp":1726993797053,"user_tz":-330,"elapsed":4384,"user":{"displayName":"Sumit Mishra","userId":"03876200325202187761"}},"outputId":"995d10a5-ba4a-4a64-a7c5-352bd8d46f29"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{\n","  \"result\": [\n","    \"What is the average total purchase amount for each customer?\",\n","    \"Which customer has the highest total purchase amount?\",\n","    \"How many unique tracks has each customer purchased?\",\n","    \"Which customer has purchased the most unique tracks?\",\n","    \"What is the average number of tracks purchased per invoice?\",\n","    \"Which album has the highest number of tracks purchased?\",\n","    \"What is the average duration of tracks purchased by each customer?\",\n","    \"Which customer has purchased tracks with the longest duration on average?\",\n","    \"What is the total revenue generated by each genre?\",\n","    \"Which genre has generated the most revenue?\",\n","    \"What is the average purchase amount for each employee?\",\n","    \"Which employee has the highest average purchase amount?\",\n","    \"How does the total purchase amount vary by country?\",\n","    \"Which country has the highest total purchase amount?\",\n","    \"What is the total number of customers in each country?\",\n","    \"Which country has the most customers?\",\n","    \"What is the average number of purchases made by each customer?\",\n","    \"Which customer has made the most purchases on average?\",\n","    \"What is the average number of days between purchases for each customer?\",\n","    \"Which customer has the shortest average time between purchases?\",\n","    \"What is the total number of tracks purchased by each customer?\",\n","    \"Which customer has purchased the most tracks?\",\n","    \"What is the average number of unique genres purchased by each customer?\",\n","    \"Which customer has purchased tracks from the most unique genres?\"\n","  ]\n","}\n"]}]},{"cell_type":"markdown","source":["Inserting dataset into opik dataset."],"metadata":{"id":"0pki0OX_LBkg"}},{"cell_type":"code","source":["# Create the synthetic dataset\n","import opik\n","from opik import DatasetItem\n","\n","synthetic_questions = json.loads(completion.choices[0].message.content)[\"result\"]\n","\n","client = opik.Opik()\n","try:\n","    dataset = client.create_dataset(name=\"synthetic_questions\")\n","    dataset.insert([\n","        DatasetItem(input={\"question\": question}) for question in synthetic_questions\n","    ])\n","except opik.rest_api.core.ApiError as e:\n","    print(\"Dataset already exists\")"],"metadata":{"id":"EKfeYhFE5f6R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Creating a LangChain Chain\n"],"metadata":{"id":"dcvzEhxDKP7n"}},{"cell_type":"markdown","source":["Next, we will create a chain that converts these natural language questions into SQL queries. This is accomplished using the create_sql_query_chain function from the LangChain library."],"metadata":{"id":"Jicbs0LkKh1G"}},{"cell_type":"code","source":["# Use langchain to create a SQL query to answer the question\n","from langchain.chains import create_sql_query_chain\n","from langchain_openai import ChatOpenAI\n","from opik.integrations.langchain import OpikTracer\n","\n","opik_tracer = OpikTracer(tags=[\"simple_chain\"])\n","\n","llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n","chain = create_sql_query_chain(llm, db).with_config({\"callbacks\": [opik_tracer]})\n","response = chain.invoke({\"question\": \"How many employees are there ?\"})\n","response\n","\n","print(response)"],"metadata":{"id":"mnsS-mvsEaMg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1726993818166,"user_tz":-330,"elapsed":1026,"user":{"displayName":"Sumit Mishra","userId":"03876200325202187761"}},"outputId":"1d09825f-1998-45ad-ac12-4f84255dbb62"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["SELECT COUNT(\"EmployeeId\") AS \"TotalEmployees\" FROM \"Employee\"\n"]}]},{"cell_type":"markdown","source":["#### Automating the Evaluation"],"metadata":{"id":"9UM0osTVKUWv"}},{"cell_type":"markdown","source":["To verify that our application is functioning correctly, we will evaluate the generated SQL queries against our synthetic dataset. This ensures the queries are valid and produce the expected results.\n","\n","Evaluate Queries: Utilize the evaluate function from the opik library to assess the validity of the SQL queries."],"metadata":{"id":"Wn4-LbGvKang"}},{"cell_type":"code","source":["from opik import Opik, track\n","from opik.evaluation import evaluate\n","from opik.evaluation.metrics import base_metric, score_result\n","from typing import Any\n","\n","class ValidSQLQuery(base_metric.BaseMetric):\n","    def __init__(self, name: str, db: Any):\n","        self.name = name\n","        self.db = db\n","\n","    def score(self, output: str, **ignored_kwargs: Any):\n","        # Add you logic here\n","\n","        try:\n","            db.run(output)\n","            return score_result.ScoreResult(\n","                name=self.name,\n","                value=1,\n","                reason=\"Query ran successfully\"\n","            )\n","        except Exception as e:\n","            return score_result.ScoreResult(\n","                name=self.name,\n","                value=0,\n","                reason=str(e)\n","            )\n","\n","valid_sql_query = ValidSQLQuery(name=\"valid_sql_query\", db=db)\n","\n","client = Opik()\n","dataset = client.get_dataset(\"synthetic_questions\")\n","\n","@track()\n","def llm_chain(input: str) -> str:\n","    response = chain.invoke({\"question\": input})\n","\n","    return response\n","\n","def evaluation_task(item):\n","    response = llm_chain(item.input[\"question\"])\n","\n","    return {\n","        \"reference\": \"hello\",\n","        \"output\": response,\n","    }\n","\n","res = evaluate(\n","    experiment_name=\"SQL question answering\",\n","    dataset=dataset,\n","    task=evaluation_task,\n","    scoring_metrics=[valid_sql_query]\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3uzgabi83quG","executionInfo":{"status":"ok","timestamp":1726994122621,"user_tz":-330,"elapsed":516,"user":{"displayName":"Sumit Mishra","userId":"03876200325202187761"}},"outputId":"0b330f04-be63-4d2e-c1b3-24d8a4525ff8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Evaluation: 100%|██████████| 24/24 [00:02<00:00,  9.98it/s]\n","╭─ synthetic_questions (24 samples) ─╮\n","│                                    │\n","│ Total time:        00:00:03        │\n","│ Number of samples: 24              │\n","│                                    │\n","│ valid_sql_query: 0.9167 (avg)      │\n","│                                    │\n","╰────────────────────────────────────╯\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"HAIXPUbXLbzb"}}]}