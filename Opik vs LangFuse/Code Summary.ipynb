{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN0hPAgUdj4BFLPs3lQWARz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Qawat6v0i1rA"},"outputs":[],"source":[]},{"cell_type":"markdown","source":["Here’s the detailed breakdown of the code:\n","\n","---\n","\n","### Code Block 1:\n","```python\n","from langchain_core.documents import Document\n","from langchain_community.vectorstores import FAISS\n","from langchain_openai import ChatOpenAI\n","from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n","from langchain_core.output_parsers import StrOutputParser, BaseOutputParser\n","```\n","\n","**Explanation 1:**\n","This block imports several key components from the Langchain framework:\n","- `Document`: Used to represent documents for processing.\n","- `FAISS`: A library for fast vector search, used to store and search embeddings.\n","- `ChatOpenAI`: The interface for interacting with the OpenAI API (e.g., GPT-3.5-turbo).\n","- `PromptTemplate` and `ChatPromptTemplate`: These help structure the prompts sent to the language model.\n","- `StrOutputParser`, `BaseOutputParser`: Handle the formatting of the output from the language model.\n","\n","---\n","\n","### Code Block 2:\n","```python\n","os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Open AI API Key:\")\n","```\n","\n","**Explanation 2:**\n","This line sets up an environment variable for the OpenAI API key. `getpass.getpass` is used to securely input the API key without exposing it in the code.\n","\n","---\n","\n","### Code Block 3:\n","```python\n","embeddings = HuggingFaceBgeEmbeddings(\n","    model_name = MODEL_NAME,\n","    model_kwargs = {'device': 'cuda'},\n","    encode_kwargs = {'normalize_embeddings': True}\n",")\n","```\n","\n","**Explanation 3:**\n","This initializes the embeddings model using Hugging Face's `bge-base-en-v1.5` model. The embeddings will be normalized, and the computation will be done on a CUDA device (GPU), ensuring faster processing.\n","\n","---\n","\n","### Code Block 4:\n","```python\n","class Preprocessing:\n","    def loadDocumentFromWeb(self, url: str) -> List[Document]:\n","        headers = {'User-Agent': USER_AGENT}\n","        return WebBaseLoader(url, header_template=headers).load()\n","```\n","\n","**Explanation 4:**\n","The `Preprocessing` class has a method `loadDocumentFromWeb` which fetches a document from a given URL using a user-agent string for web scraping. It returns a list of `Document` objects.\n","\n","---\n","\n","### Code Block 5:\n","```python\n","class Director:\n","    name: str\n","    linkedin_handle: str\n","    education: List[str]\n","```\n","\n","**Explanation 5:**\n","This defines a simple data structure (`Director`) to represent a company director, including fields for their name, LinkedIn handle, and education.\n","\n","---\n","\n","### Code Block 6:\n","```python\n","class Companies(BaseEntityStore):\n","    store: Dict[str, str] = {}\n","    \n","    def get(self, key: str) -> Optional[str]:\n","        return self.store.get(key, None)\n","```\n","\n","**Explanation 6:**\n","`Companies` is a class that extends `BaseEntityStore` to store company-related data. It includes methods for retrieving, storing, and updating company data in a dictionary (`store`).\n","\n","---\n","\n","### Code Block 7:\n","```python\n","class RAG:\n","    def createVectorStore(self):\n","        self.vectorStore = FAISS.from_texts([\"\"], self.embeddings)\n","        self.vectorStore.save_local(VECTOR_STORE_PATH)\n","```\n","\n","**Explanation 7:**\n","This is part of the `RAG` (Retrieval-Augmented Generation) class. The `createVectorStore` method initializes a FAISS vector store and saves it locally for future use. FAISS is used to store and search vector embeddings efficiently.\n","\n","---\n","\n","### Code Block 8:\n","```python\n","class QueryDecomposer:\n","    def createDecomposerChain(self):\n","        self.queryAnalyzer = self.prompt | self.llm | self.parser\n","```\n","\n","**Explanation 8:**\n","`QueryDecomposer` creates a chain where the user's query goes through a prompt template, then to the language model, and finally to the parser to break it down into smaller subqueries.\n","\n","---\n","\n","### Code Block 9:\n","```python\n","class LinkedinHandles:\n","    def getLinkedinHandle(self, name: str) -> Optional[str]:\n","        query = f'site:linkedin.com/in/ \"{name}\"'\n","        results = self.search.results(query)\n","        return results.get(\"organic_results\")[0].get(\"link\", None)\n","```\n","\n","**Explanation 9:**\n","The `LinkedinHandles` class contains a method to search for a LinkedIn profile URL based on a director's name. It performs a Google search and returns the first LinkedIn profile link it finds.\n","\n","---\n","\n","### Code Block 10:\n","```python\n","class REACTwithReflexion:\n","    def addTool(self, tool):\n","        self.tools.append(tool)\n","        self.updateChain()\n","```\n","\n","**Explanation 10:**\n","`REACTwithReflexion` is a class that manages the tools (e.g., LinkedIn search, vector retrieval) used by the REACT agent. The `addTool` method allows for adding new tools to the REACT system and updating the agent with new capabilities.\n","\n","---\n","\n","### Summary:\n","This code sets up a **Retrieval-Augmented Generation (RAG)** framework that uses the **Langchain** ecosystem to perform complex tasks like retrieving documents, processing them, and using AI to generate responses. It involves:\n","1. Setting up embeddings using a Hugging Face model.\n","2. Web scraping documents and chunking them for vectorization.\n","3. Storing and retrieving company and director data.\n","4. Implementing a system that can analyze the independence of company directors, fetch their LinkedIn profiles, and extract professional details like education and work history.\n","5. Using a query decomposition system to break down complex queries into smaller parts for better information retrieval."],"metadata":{"id":"XsLrjVV1i3YJ"}},{"cell_type":"code","source":[],"metadata":{"id":"xdKU5lA5i320"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This code is a complex implementation of a retrieval-augmented generation (RAG) system using Langchain and LangSmith. It integrates various tools and frameworks to perform tasks related to analyzing the background independence of company directors. Let’s break it down step by step:\n","\n","### 1. **Setup and Libraries:**\n","   - The necessary Python libraries are imported, including tools for loading documents, creating vector stores, performing web scraping, and accessing APIs like OpenAI and SerpAPI.\n","   - Keys for various services like OpenAI, Cohere, Langchain, SerpAPI, and ProxyCurl are stored as environment variables.\n","\n","### 2. **Model and Vector Store Setup:**\n","   - **Language Model:** GPT-3.5-turbo is used as the primary language model for generating answers to queries.\n","   - **Embeddings:** HuggingFace's BGE model is used to create text embeddings. These embeddings are essential for the RAG framework, as they allow for similarity searches in a vector space.\n","   - **Vector Store:** FAISS (Facebook AI Similarity Search) is used as the vector store. Documents will be split into chunks, embedded, and stored in this vector store for retrieval.\n","\n","### 3. **Preprocessing:**\n","   - The `Preprocessing` class provides methods to load documents from the web using `WebBaseLoader` and split them into chunks using `RecursiveCharacterTextSplitter`. This prepares the data for further processing.\n","   - **Chunking:** Documents are split into chunks of 500 characters with 50 characters overlapping between chunks. This overlap ensures that the context is not lost between chunks.\n","\n","### 4. **Data Model Classes:**\n","   - **Director and Company Data Classes:** These are simple data classes used to represent the structure of directors and companies. A `Company` contains a list of `Director` objects.\n","   - **Company Store (Local Entity Store):** The `Companies` class stores and retrieves company data locally. It handles operations like storing, updating, deleting, and retrieving companies and their associated directors.\n","\n","### 5. **RAG (Retrieval-Augmented Generation) Framework:**\n","   - The `RAG` class encapsulates the logic for the RAG system. It:\n","     - Initializes the LLM and embeddings model.\n","     - Creates a FAISS-based vector store for document retrieval.\n","     - Processes companies by loading their SEC 10-K filings, extracting important sections (e.g., signatures), and storing them in the vector store.\n","     - Adds documents to the vector store and manages the company metadata, including the snippets of text related to the companies.\n","   \n","### 6. **Reranker and Query Decomposer:**\n","   - **Reranker:** The `RerankerRAG` class uses Cohere’s reranking model to improve the quality of search results by ranking the top N results.\n","   - **Query Decomposer:** The `QueryDecomposer` class decomposes a complex question into specific sub-queries using GPT-3.5-turbo. These sub-queries are aimed at addressing different aspects of the original question, ensuring more comprehensive retrieval from the vector store.\n","\n","### 7. **Tools for Director Analysis:**\n","   - **Name Extraction:** The `nameExtractor` class extracts the names of company directors from a text snippet using a prompt chain.\n","   - **LinkedIn Handles:** The `LinkedinHandles` class searches for LinkedIn profiles of directors using the SerpAPI, retrieving LinkedIn handles based on director names.\n","   - **ProxyCurl Integration:** The `proxyCurl` class fetches detailed LinkedIn data (education, work history) using the ProxyCurl API.\n","   - **Director Background Tool:** This tool fetches the education and career information of directors using their LinkedIn profiles and stores it in the local company store.\n","\n","### 8. **REACT with Reflexion:**\n","   - The `REACTwithReflexion` class integrates a reflection-based approach for performing an in-depth analysis of directors’ background independence.\n","   - **REACT Agent:** This agent runs a cycle of Thought → Action → Observation → Reflection. It performs multiple steps:\n","     1. Processes the user’s input.\n","     2. Uses tools (e.g., LinkedIn handle retrieval, director background check) to gather relevant data.\n","     3. Reflects on the gathered information to determine whether the directors have independent backgrounds.\n","   - **Prompt Template:** A predefined prompt template is used to guide the agent in conducting the analysis and reflecting on the observations.\n","   \n","### 9. **Executing the RAG Framework:**\n","   - After initializing the RAG system, documents from Tesla and GM’s 10-K filings are added to the vector store.\n","   - A reranker and decomposer are used to create a RAG chain that enhances the retrieval process by decomposing complex queries into sub-queries and reranking the search results.\n","   - The agent then invokes the REACT-based reflexion system to analyze the independence of the directors' backgrounds, ensuring that it strictly uses retrieved data and provides a final well-supported answer.\n","\n","### 10. **Output Example:**\n","   - The final command demonstrates how the system is used to analyze the background independence of Tesla's directors by retrieving information from 10-K filings, LinkedIn profiles, and other available sources.\n","\n","In summary, this code creates a powerful RAG system that analyzes the independence of company directors’ backgrounds using data from financial filings and LinkedIn profiles. It incorporates a query decomposition mechanism, reranking of search results, and a reflexion-based agent that ensures the reliability and accuracy of the analysis."],"metadata":{"id":"puDtw3T1jRoG"}},{"cell_type":"code","source":[],"metadata":{"id":"UuwDzxeQjR-i"},"execution_count":null,"outputs":[]}]}